{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'factorial'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-258fd6acd2ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscikitplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mskplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeaturetools\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mft\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\site-packages\\statsmodels\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrobust\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrobust\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrobust_linear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRLM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m from .discrete.discrete_model import (Poisson, Logit, Probit,\n\u001b[0m\u001b[0;32m     17\u001b[0m                                       \u001b[0mMNLogit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNegativeBinomial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                                       \u001b[0mGeneralizedPoisson\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1_slsqp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfit_l1_slsqp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenpoisson_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\site-packages\\statsmodels\\distributions\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mempirical_distribution\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mECDF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonotone_fn_inverter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStepFunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0medgeworth\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExpandedNormal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdiscrete\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenpoisson_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzipoisson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzigenpoisson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzinegbin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python36\\lib\\site-packages\\statsmodels\\distributions\\edgeworth.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolynomial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhermite_e\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHermiteE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfactorial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrv_continuous\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'factorial'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sns\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno\n",
    "import scipy.stats as st\n",
    "import scikitplot as skplt\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "import featuretools as ft\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder, KBinsDiscretizer\n",
    "from sklearn.metrics import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "#classifiers\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, Ridge, Lasso\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Load and Preview\n",
    "df = pd.read_csv('adult.csv')\n",
    "display(df.shape)\n",
    "display(df.head())\n",
    "display(df.tail())\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EXPLORATORY ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    display(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df.head(500), figsize = (15,9), diagonal = 'hist' )\n",
    "plt.xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((a,b),(c,d),(e,f)) = plt.subplots(3,2,figsize=(15,20))\n",
    "plt.xticks(rotation=45)\n",
    "sns.countplot(df['workclass'],hue=df['income'],ax=f)\n",
    "sns.countplot(df['relationship'],hue=df['income'],ax=b)\n",
    "sns.countplot(df['marital.status'],hue=df['income'],ax=c)\n",
    "sns.countplot(df['race'],hue=df['income'],ax=d)\n",
    "sns.countplot(df['sex'],hue=df['income'],ax=e)\n",
    "sns.countplot(df['native.country'],hue=df['income'],ax=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (a,b)= plt.subplots(1,2,figsize=(20,6))\n",
    "sns.boxplot(y='hours.per.week',x='income',data=df,ax=a)\n",
    "sns.boxplot(y='age',x='income',data=df,ax=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manipulating data to see it through different perspectives\n",
    "\n",
    "#df.sort_values(['age','fnlwgt'], ascending = False).head(20)\n",
    "#df.groupby(df.age).count().plot(kind = 'bar')\n",
    "#df.groupby('age')['fnlwgt'].mean().sort_values(ascending = False).head(10)\n",
    "#pd.crosstab(df.age, df.fnlwgt).apply(lambda x: x/x.sum(), axis=1)\n",
    "#df['young_male'] = ((df.fnlwgt == '19302') & (df.age < 30)).map({True: 'young male', False: 'other'})\n",
    "#display(df['young_male'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA CLEANING**\n",
    "- Duplicates\n",
    "- Non-sense values or errors\n",
    "- Missing values\n",
    "- Scaling\n",
    "- Data type conversions\n",
    "- Randomize data \n",
    "- Class imbalances \n",
    "- Outliers\n",
    "- Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Nonsense values**\n",
    "Negative Values, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "for col in df:\n",
    "    print(col)\n",
    "    print (df[col].unique())\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe(include = 'all')) #it helps to understand non-sense values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK FOR NEGATIVE VALUES BY COLORING THEM\n",
    "def color_negative_red(val):\n",
    "    color = 'red' if val <= 0 else 'black'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "test = df[['age','capital.gain', 'capital.loss', 'education.num', 'fnlwgt']]\n",
    "colored = test.head().style.applymap(color_negative_red)\n",
    "\n",
    "display(colored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Missing Values**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingno.matrix(df, figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('adult.csv')\n",
    "\n",
    "#convert '?' to NaN\n",
    "df[df == '?'] = np.nan\n",
    "\n",
    "#fill the NaN values with the mode. It could be filled with median, mean, etc\n",
    "for col in df.columns:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "display(df.head())    \n",
    "    \n",
    "#we could fill every missing values with medians of the columns\n",
    "#df = data.fillna(df.median())\n",
    "\n",
    "#also, instead of fill, we can remove every row in which 10% of the values are missing\n",
    "# df = df.loc[df.isnull().mean(axis=1) < 0.1]    \n",
    "    \n",
    "\"\"\"\n",
    "#It is possible to use an imputer. and fill with the values of median, mean, etc\n",
    "#If imputation technique is used, it is a good practice to add an additional binary feature as a missing indicator.\n",
    "imputer = SimpleImputer(strategy='most_frequent', missing_values=np.nan,  add_indicator=True)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer = SimpleImputer(strategy='constant')\n",
    "df = pd.DataFrame(imputer.fit_transform(df))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# It is also possible to run a bivariate imputer (iterative imputer). However, it is needed to do labelencoding first. The code below enables us to run the imputer with a Random Forest estimator\n",
    "# The Iterative Imputer is developed by Scikit-Learn and models each feature with missing values as a function of other features. It uses that as an estimate for imputation. At each step, a feature is selected as output y and all other features are treated as inputs X. A regressor is then fitted on X and y and used to predict the missing values of y. This is done for each feature and repeated for several imputation rounds.\n",
    "# The great thing about this method is that it allows you to use an estimator of your choosing. I used a RandomForestRegressor to mimic the behavior of the frequently used missForest in R.\n",
    "imp = IterativeImputer(RandomForestRegressor(), max_iter=10, random_state=0)\n",
    "df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "\n",
    "#If you have sufficient data, then it might be an attractive option to simply delete samples with missing data. However, keep in mind that it could create bias in your data. Perhaps the missing data follows a pattern that you miss out on.\n",
    "\n",
    "#The Iterative Imputer allows for different estimators to be used. After some testing, I found out that you can even use Catboost as an estimator! Unfortunately, LightGBM and XGBoost do not work since their random state names differ.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(more examples of operations for replacing values if needed in the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Replace specific values if needed\n",
    "Replace symbol in whole column\n",
    "df['age'] = df['age'].str.replace('–', '**', regex = True)\n",
    "\n",
    "\n",
    "#DROP multiple columns\n",
    "df.drop(['age' , 'fnlwgt'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "#Find text with regex and replace with nothing\n",
    " df = df.replace({\n",
    "     'age':'[A-Za-z]', \n",
    "     'fnlwgt': '[A-Za-z]',\n",
    " },'',regex = True)\n",
    "\n",
    "\n",
    "#Example of applying a formula to entire column\n",
    "\n",
    " def euro(cell):\n",
    "     cell = cell.strip('€')\n",
    "     return cell\n",
    " df.Wage = df.Wage.apply(euro)\n",
    "\n",
    "#Insert value in cell depending on values from other cells\n",
    " def impute_age(cols):\n",
    "     age = cols[0]\n",
    "     Pclass = cols[1]\n",
    "     if pd.isnull(Age):\n",
    "         if Pclass == 1:\n",
    "             return 37\n",
    "         else:\n",
    "             return 24\n",
    "     else:\n",
    "         return Age\n",
    "         \n",
    "#CHANGING DATA TYPES\n",
    "#changing values to float\n",
    "df[['Value','Wage','Age']].apply(pd.to_numeric, errors = 'coerce')\n",
    "df.column.astype(float)\n",
    "df.dtypes\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Duplicates **\n",
    "We can confidently remove duplicates now, because when we'll do the oversampling we we'll use the Smote method that won't generate more duplicates (in contrast to RandomOverSampling). Also, by doing this before the oversampling, we are guaranteeing that we'll have exactly 50%-50% of y-labels balance later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().sum()) #check if there are duplicates\n",
    "df.drop_duplicates(keep = 'first', inplace = True) #get rid of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation/Creation\n",
    "- Feature Transformation (Modidy existing features) -> Scaling, normalize, standarize, logarithim, ...\n",
    "- Feature Creation (Add useful features) -> Modify to new, Combine features, Cluster some feature, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning\n",
    "Binning continuous variables prevents overfitting which is a common problem for tree based models like decision trees and random forest. It also helps dealing with outliers.\n",
    "\n",
    "Let's binning the age and see later if the results improved or not. \n",
    "\n",
    "Binning with fixed-width intervals (good for uniform distributions). For skewed distributions (not normal), we can use binning adaptive (quantile based)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Binner adaptive\n",
    "binner = KBinsDiscretizer(encode='ordinal')\n",
    "binner.fit(df[['age']])\n",
    "df['age'] = binner.transform(df[['age']])\n",
    "\n",
    "\"\"\"\n",
    "#Using fixed-width intervals\n",
    "age_labels = ['infant','child','teenager','young_adult','adult','old', 'very_old']\n",
    "\n",
    "ranges = [0,5,12,18,35,60,81,100]\n",
    "\n",
    "df['age'] = pd.cut(df.age, ranges, labels = age_labels)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Label Encoding **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['age', 'workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
    "numerical = ['fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using labelencoder\n",
    "le = LabelEncoder()\n",
    "for feature in categorical:\n",
    "        df[feature] = le.fit_transform(df[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Finding Outliers **\n",
    "\n",
    "We can detect outliers in 3 ways:\n",
    "\n",
    "- Standard Deviation\n",
    "- Percentiles (Tukey method)\n",
    "- Isolation Forest or LocalOutlierFactor (more appropriate for Anomaly/Fraud Detection Problems)\n",
    "\n",
    "Then, we can handle them by:\n",
    " - Remove them\n",
    " - Change them to max/min limit\n",
    " \n",
    "The definition of outlier is quite dubious, but we can defined them as those values that surpasse the limit of 1.5 * IQR.\n",
    "In this case, either the standard deviation method or Tukey method are valid options. We just need to try and see which gives better results (if it produces better results at all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See outliers through boxplots\n",
    "df.plot(kind = 'box', sharex = False, sharey = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tukey Method\n",
    "\n",
    "n = 2 #In this case, we considered outliers as rows that have at least two outlied numerical values. The optimal value for this parameter can be later determined though the cross-validation\n",
    "indexes = []\n",
    "\n",
    "for col in df.columns[0:14]:\n",
    "    Q1 = np.percentile(df[col], 25)\n",
    "    Q3 = np.percentile(df[col],75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    limit = 1.5 * IQR\n",
    "\n",
    "    list_outliers = df[(df[col] < Q1 - limit) | (df[col] > Q3 + limit )].index # Determine a list of indices of outliers for feature col\n",
    "    \n",
    "    indexes.extend(list_outliers) # append the found outlier indices for col to the list of outlier indices \n",
    "\n",
    "indexes = Counter(indexes)        \n",
    "multiple_outliers = list( k for k, v in indexes.items() if v > n )\n",
    "\n",
    "df.drop(multiple_outliers, axis = 0)\n",
    "\n",
    "df = df.drop(multiple_outliers, axis = 0).reset_index(drop=True)\n",
    "print(str(len(multiple_outliers)) + \" outliers were eliminated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Setting the min/max to outliers using standard deviation\n",
    "for col in df.columns[0:14]:\n",
    "    factor = 3 #The optimal value for this parameter can be later determined though the cross-validation\n",
    "    upper_lim = df[col].mean () + df[col].std () * factor\n",
    "    lower_lim = df[col].mean () - df[col].std () * factor\n",
    "\n",
    "    df = df[(df[col] < upper_lim) & (df[col] > lower_lim)]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Dealing with imbalanced data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there are labels imbalance\n",
    "sns.countplot(x=df['income'],palette='RdBu_r')\n",
    "df.groupby('income').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['income'] = df['income'].map({'>50K': 1, '<=50K': 0})\n",
    "\n",
    "Y = df['income']\n",
    "X = df.drop('income',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** oversampling the data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling using smote \n",
    "smk = SMOTETomek(random_state = 42)\n",
    "X, Y = smk.fit_sample(X, Y)\n",
    "\n",
    "\"\"\"\n",
    "# RANDOM Oversample #####\n",
    "os = RandomOverSampler() #ratio of one feature and another is 50% and 50% respectively\n",
    "X, Y = os.fit_sample(X, Y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If dowsample was needed...\n",
    "\n",
    "\"\"\"\n",
    "# RANDOM Undersample #####\n",
    "os = RandomUnderSampler() #ratio of one feature and another is 50% and 50% respectively\n",
    "X, Y = os.fit_sample(X, Y)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "#Using nearmiss\n",
    "nm = NearMiss()\n",
    "X, Y = nm.fit_sample(X, Y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#making sure that everything is good now\n",
    "df = pd.concat([X, Y], axis=1)\n",
    "sns.countplot(x=Y,palette='RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Skewed Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyze_skew():\n",
    "    #analyze which features are skewed\n",
    "    fig = plt.figure(figsize = (15,10))\n",
    "    cols = 3\n",
    "    rows = math.ceil(float(df[numerical].shape[1] / cols))\n",
    "    for i, column in enumerate(numerical):\n",
    "        ax = fig.add_subplot(rows, cols, i + 1)\n",
    "        ax.set_title(column)\n",
    "        if df.dtypes[column] == np.object:\n",
    "            df[column].value_counts().plot(kind = 'bar', axes = ax)\n",
    "        else:\n",
    "            df[column].hist(axes = ax)\n",
    "            plt.xticks(rotation = 'vertical')\n",
    "    plt.subplots_adjust(hspace = 0.7, wspace = 0.2)\n",
    "    plt.show()\n",
    "    \n",
    "    # from the plots we can see that there are multiple features skewed. However, the following procedure allows us to see in detail how skewed are they.\n",
    "    skew_feats = df[numerical].skew().sort_values(ascending=False)\n",
    "    skewness = pd.DataFrame({'Skew': skew_feats})\n",
    "    \n",
    "    display(skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's reduce the skew of fnlwgt, capital.gain, capital.loss\n",
    "skewed = ['fnlwgt', 'capital.gain', 'capital.loss']\n",
    "features_log_transformed = pd.DataFrame(data=df)\n",
    "features_log_transformed[skewed] = df[skewed].apply(lambda x: np.log(x + 1)) #it can be other function like polynomial, but generally the log funtion is suitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check again if is everything ok\n",
    "analyze_skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Selection**\n",
    "- Feature Selection/Reduction (Remove useless features) -> See feature importance, correlations, Dimensionality reduction,\n",
    "\n",
    "As we only have 14 features, we are not pressured to make a feature selection/reduction in order to increase drastically the computing time of the algorithms. So, for now, we are going to investigate if there are features extremely correlated to each other. After tuning and choosing the best model, we are revisiting feature selection methods just in case we face overfitting or to see if we could achieve the same results with the chosen model but with fewer features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Correlation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple regression line of various variables in relation to  one other\n",
    "#sns.pairplot(X, x_vars = ['capital.loss', 'hours.per.week', 'education.num'], y_vars = 'age', size = 7, aspect = 0.7, kind = 'reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between variables each other. It may take a while, not advised if there are plenty of features\n",
    "#sns.pairplot(data = X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,7))\n",
    "sns.heatmap(X.corr(method = 'pearson'),annot=True,cmap='coolwarm') # the method can also be 'spearman' or kendall'\n",
    "\n",
    "#to see the correlation between just two variables\n",
    "#df['age'].corr(df['capital.gain']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Generally all of the features are not highly correlated (no correlations above abs(0.5)). The strongest correlations here is \"sex\"vs\"relationship\". Down bellow are the procedures to drop the highest correlated features, even though it's not needed in this case or it might even result in worse results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Base Line Model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=pd.DataFrame(Y)\n",
    "l['baseline'] = 0\n",
    "k = pd.DataFrame(confusion_matrix(Y,l['baseline']))\n",
    "print(k)\n",
    "print(\"Accuracy: \" + str(accuracy_score(Y, l['baseline'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** One Hot Encoding **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_feat = pd.DataFrame(OneHotEncoder(categories=\"auto\").fit_transform(df[categorical]).toarray())\n",
    "\n",
    "#concatenate without adding null values -.-''\n",
    "l1=encoded_feat.values.tolist()\n",
    "l2=df[numerical].values.tolist()\n",
    "for i in range(len(l1)):\n",
    "    l1[i].extend(l2[i])\n",
    "\n",
    "X=pd.DataFrame(l1,columns=encoded_feat.columns.tolist()+df[numerical].columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Scaling Normalization**\n",
    "\n",
    "StandardScaler removes the mean and scales the data to unit variance. However, the outliers have an influence when computing the empirical mean and standard deviation which shrink the range of the feature values as shown in the left figure below. Note in particular that because the outliers on each feature have different magnitudes, the spread of the transformed data on each feature is very different: most of the data lie in the [-2, 4] range for the transformed median income feature while the same data is squeezed in the smaller [-0.2, 0.2] range for the transformed number of households.\n",
    "\n",
    "StandardScaler therefore cannot guarantee balanced feature scales in the presence of outliers.\n",
    "\n",
    "MinMaxScaler rescales the data set such that all feature values are in the range [0, 1] as shown in the right panel below. However, this scaling compress all inliers in the narrow range [0, 0.005] for the transformed number of households.\n",
    "\n",
    "[Source](https://stackoverflow.com/questions/51237635/difference-between-standard-scaler-and-minmaxscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard\n",
    "col_names = X.columns\n",
    "\n",
    "features = X[col_names]\n",
    "\n",
    "scaler = StandardScaler().fit(features.values)\n",
    "features = scaler.transform(features.values)\n",
    "\n",
    "X[col_names] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Minimax\n",
    "col_names = X.columns\n",
    "\n",
    "features = X[col_names]\n",
    "\n",
    "scaler = MinMaxScaler().fit(features.values)\n",
    "features = scaler.transform(features.values)\n",
    "\n",
    "X[col_names] = features\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Split Data **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle before split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=X.values.tolist()\n",
    "l2=pd.DataFrame(Y).values.tolist()\n",
    "for i in range(len(l1)):\n",
    "    l1[i].extend(l2[i])\n",
    "\n",
    "new_df=pd.DataFrame(l1,columns=X.columns.tolist()+pd.DataFrame(Y).columns.tolist())\n",
    "\n",
    "new_df = shuffle(new_df, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(new_df,test_size=0.2)\n",
    "\n",
    "y_train = train['income']\n",
    "X_train = train.drop('income',axis=1)\n",
    "y_test = test['income']\n",
    "X_test = test.drop('income',axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logistic Regression (all features)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "\n",
    "model = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "y_score = model.decision_function(X_test)\n",
    "\n",
    "\n",
    "k = pd.DataFrame(confusion_matrix(y_test,y_pred))\n",
    "print(k)\n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred,average='binary')\n",
    "\n",
    "print('Precision: {} / Recall: {}'.format(round(precision, 3),\n",
    "                                                        round(recall, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ROC CURVE AND AUC **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,the = roc_curve(y_test,y_score)\n",
    "roc_auc_score(y_test,y_pred)\n",
    "plt.plot(fpr,tpr,)\n",
    "print(roc_auc_score(test['income'],y_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Decision Tree **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_features=14,min_samples_leaf=100,random_state=10)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = pd.DataFrame(confusion_matrix(y_test,y_pred))\n",
    "print(k)\n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred,average='binary')\n",
    "\n",
    "print('Precision: {} / Recall: {}'.format(round(precision, 3),\n",
    "                                                        round(recall, 3)))\n",
    "skplt.metrics.plot_confusion_matrix(y_test, y_pred, title=\"{} Confusion Matrix\".format(\"Decision Tree\"),\n",
    "            normalize=True,figsize=(6,6),text_fontsize='large')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Technique to prun decision trees **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRUNNING DECISION TREES\n",
    "#https://stackoverflow.com/questions/49428469/pruning-decision-trees\n",
    "'''Directly restricting the lowest value (number of occurences of a particular class) of a leaf\n",
    "cannot be done with min_impurity_decrease or any other built-in stopping criteria.\n",
    "I think the only way you can accomplish this without changing the source code \n",
    "of scikit-learn is to post-prune your tree. \n",
    "To accomplish this, you can just traverse the tree \n",
    "and remove all children of the nodes with minimum class count less than 5 \n",
    "(or any other condition you can think of). I will continue your example:'''\n",
    "\n",
    "from sklearn.tree._tree import TREE_LEAF\n",
    "\n",
    "def prune_index(inner_tree, index, threshold):\n",
    "    if inner_tree.value[index].min() < threshold:\n",
    "        # turn node into a leaf by \"unlinking\" its children\n",
    "        inner_tree.children_left[index] = TREE_LEAF\n",
    "        inner_tree.children_right[index] = TREE_LEAF\n",
    "    # if there are children, visit them as well\n",
    "    if inner_tree.children_left[index] != TREE_LEAF:\n",
    "        prune_index(inner_tree, inner_tree.children_left[index], threshold)\n",
    "        prune_index(inner_tree, inner_tree.children_right[index], threshold)\n",
    "\n",
    "print(sum(dt.tree_.children_left < 0))\n",
    "# start pruning from the root\n",
    "prune_index(dt.tree_, 0, 5)\n",
    "sum(dt.tree_.children_left < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** XGBoost **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier()\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = pd.DataFrame(confusion_matrix(y_test,y_pred))\n",
    "print(k)\n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred,average='binary')\n",
    "\n",
    "print('Precision: {} / Recall: {}'.format(round(precision, 3),\n",
    "                                                        round(recall, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** CATboost **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = CatBoostClassifier(learning_rate=0.04)\n",
    "\n",
    "model = clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "k = pd.DataFrame(confusion_matrix(y_test,y_pred))\n",
    "print(k)\n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred,average='binary')\n",
    "\n",
    "print('Precision: {} / Recall: {}'.format(round(precision, 3),\n",
    "                                                        round(recall, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best hyperparameters for the models under analysis\n",
    "Just tuning the crutial parameters, not all of them.\n",
    "I intercalate between randomizedSearch and GridSearch to diversify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('NB', GaussianNB()))\n",
    "tuning_num_folds = 5\n",
    "jobs=4\n",
    "num_random_state=10\n",
    "scoring_criteria='accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Tuning Logistic Regression **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_vals = [0.0001, 0.001, 0.01, 0.1,0.13,0.2, .15, .25, .275, .33, 0.5, .66, 0.75, 1.0, 2.5, 4.0,4.5,5.0,5.1,5.5,6.0, 10.0, 100.0, 1000.0]\n",
    "penalties = ['l1','l2']\n",
    "param = {'penalty': penalties, 'C': C_vals}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param,verbose=False, cv = StratifiedKFold(n_splits=tuning_num_folds,random_state=num_random_state,shuffle=True), n_jobs=jobs,scoring=scoring_criteria)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "models.append(('LR', LogisticRegression(penalty=grid.best_params_['penalty'], C=grid.best_params_['C'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** CatBoostClassifier Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'iterations': [100, 150], 'learning_rate': [0.3, 0.4, 0.5]}\n",
    "\n",
    "grid = GridSearchCV(CatBoostClassifier(), param,verbose=True, cv = StratifiedKFold(n_splits=tuning_num_folds,random_state=num_random_state,shuffle=True), n_jobs=jobs,scoring=scoring_criteria)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "models.append(('CAT', CatBoostClassifier(iterations=grid.best_params_['iterations'], learning_rate=grid.best_params_['learning_rate'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Random Forest Tuning **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params ={\n",
    "             'max_depth': st.randint(3, 11),\n",
    "            'n_estimators': [50,100,150,200,250],\n",
    "             'max_features':[\"sqrt\", \"log2\"],\n",
    "             'max_leaf_nodes':st.randint(6, 10)\n",
    "            }\n",
    "\n",
    "skf = StratifiedKFold(n_splits=tuning_num_folds, shuffle = True)\n",
    "\n",
    "random_search = RandomizedSearchCV(RandomForestClassifier(), param_distributions=params, scoring='roc_auc', n_jobs=jobs, cv=skf.split(X_train,y_train), verbose=3, random_state=num_random_state)\n",
    "random_search.fit(X_train,y_train)\n",
    "\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=random_search.best_params_['n_estimators'], max_features=random_search.best_params_['max_features'], max_leaf_nodes=random_search.best_params_['max_leaf_nodes'], max_depth=random_search.best_params_['max_depth'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** XGBClassifier Tuning **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "\n",
    "skf = StratifiedKFold(n_splits=tuning_num_folds, shuffle = True)\n",
    "\n",
    "random_search = RandomizedSearchCV(XGBClassifier(), param_distributions=params, scoring='roc_auc', n_jobs=jobs, cv=skf.split(X_train,y_train), verbose=3, random_state=num_random_state)\n",
    "random_search.fit(X_train,y_train)\n",
    "\n",
    "models.append(('XGB', XGBClassifier(colsample_bytree=random_search.best_params_['colsample_bytree'], gamma=random_search.best_params_['gamma'], max_depth=random_search.best_params_['max_depth'], min_child_weight=random_search.best_params_['min_child_weight'], subsample=random_search.best_params_['subsample'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** MLPClassifier tuning **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param ={'max_iter': np.logspace(1, 5, 10).astype(\"int32\"),\n",
    "             'hidden_layer_sizes': np.logspace(2, 3, 4).astype(\"int32\"),\n",
    "             'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
    "             'learning_rate': ['adaptive'],\n",
    "             'early_stopping': [True],\n",
    "             'alpha': np.logspace(2, 3, 4).astype(\"int32\")\n",
    "            }\n",
    "\n",
    "grid = GridSearchCV(MLPClassifier(), param,verbose=False, cv = StratifiedKFold(n_splits=tuning_num_folds,random_state=num_random_state,shuffle=True), n_jobs=jobs,scoring=scoring_criteria)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "models.append(('MLP', MLPClassifier(max_iter=grid.best_params_['max_iter'], hidden_layer_sizes=grid.best_params_['hidden_layer_sizes'], activation=grid.best_params_['activation'], early_stopping=grid.best_params_['early_stopping'], learning_rate=grid.best_params_['learning_rate'], alpha=grid.best_params_['alpha'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** KNeighborsClassifier tuning **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params ={'n_neighbors': st.randint(1,40),\n",
    "            'weights':['uniform','distance']\n",
    "            }\n",
    "\n",
    "skf = StratifiedKFold(n_splits=tuning_num_folds, shuffle = True)\n",
    "\n",
    "random_search = RandomizedSearchCV(KNeighborsClassifier(), param_distributions=params, scoring='roc_auc', n_jobs=jobs, cv=skf.split(X_train,y_train), verbose=3, random_state=num_random_state)\n",
    "random_search.fit(X_train,y_train)\n",
    "\n",
    "models.append(('KNN', KNeighborsClassifier(n_neighbors=random_search.best_params_['n_neighbors'], weights=random_search.best_params_['weights'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Linear Discriminant tuning **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param={'solver': ['svd', 'lsqr', 'eigen']}\n",
    "\n",
    "grid = GridSearchCV(LinearDiscriminantAnalysis(), param,verbose=True, cv = StratifiedKFold(n_splits=tuning_num_folds,random_state=num_random_state,shuffle=True), n_jobs=jobs,scoring=scoring_criteria)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "models.append(('LDA', LinearDiscriminantAnalysis(solver=grid.best_params_['solver'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** DecisionTreeClassifier tuning **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'min_samples_split': [4,7,10,12]}\n",
    "\n",
    "grid = GridSearchCV(DecisionTreeClassifier(), param,verbose=True, cv = StratifiedKFold(n_splits=tuning_num_folds,random_state=num_random_state,shuffle=True), n_jobs=jobs,scoring=scoring_criteria)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "models.append(('DTC', DecisionTreeClassifier(min_samples_split=grid.best_params_['min_samples_split'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** SVC tuning **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param={'C':[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5],\n",
    "      'kernel': [\"linear\",\"rbf\"]\n",
    "      }\n",
    "\n",
    "grid = GridSearchCV(SVC(), param,verbose=True, cv = StratifiedKFold(n_splits=tuning_num_folds,random_state=num_random_state,shuffle=True), n_jobs=jobs,scoring=scoring_criteria)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "models.append(('SVC', SVC(C=grid.best_params_['C'], kernel=grid.best_params_['kernel'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** SGDClassifier tuning **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param ={'loss':[\"hinge\",\"log\",\"modified_huber\",\"epsilon_insensitive\",\"squared_epsilon_insensitive\"]}\n",
    "\n",
    "grid = GridSearchCV(SGDClassifier(), param,verbose=False, cv = StratifiedKFold(n_splits=tuning_num_folds,random_state=num_random_state,shuffle=True), n_jobs=jobs,scoring=scoring_criteria)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "models.append(('SGD', SGDClassifier(loss=grid.best_params_['loss'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** GradientBoostingClassifier tuning **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid ={'n_estimators': st.randint(100, 800),\n",
    "            'loss': ['deviance', 'exponential'],\n",
    "            'learning_rate': [0.1, 0.01,0.05,0.001],\n",
    "            'max_depth': np.arange(2, 12, 1)}\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(GradientBoostingClassifier(), param_distributions=params, scoring='roc_auc', n_jobs=jobs, cv=skf.split(X_train,y_train), verbose=3, random_state=num_random_state)\n",
    "random_search.fit(X_train,y_train)\n",
    "\n",
    "models.append(('GraBoost', GradientBoostingClassifier(n_estimators=random_search.best_params_['n_estimators'], loss=random_search.best_params_['loss'], max_depth=random_search.best_params_['max_depth'], learning_rate=random_search.best_params_['learning_rate'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** AdaBoostClassifier tuning **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params ={'n_estimators':st.randint(100, 800),\n",
    "        'learning_rate':np.arange(.1, 4, .5)}\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(AdaBoostClassifier(), param_distributions=params, scoring='roc_auc', n_jobs=jobs, cv=skf.split(X_train,y_train), verbose=3, random_state=num_random_state)\n",
    "random_search.fit(X_train,y_train)\n",
    "\n",
    "models.append(('Ada', AdaBoostClassifier(n_estimators=random_search.best_params_['n_estimators'], learning_rate=random_search.best_params_['learning_rate'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 0.20\n",
    "seed = 7\n",
    "num_folds = 10\n",
    "scoring = 'accuracy'\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X,Y,\n",
    "    test_size=validation_size,random_state=seed)\n",
    "\n",
    "\n",
    "# evalutate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble, voting, bagging, boosting, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical tests, Comparing, Ranking, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle('Algorith Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking overfitting/underfiiting. Another Interation on Feature Selection/Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter Method - easiest but not the best way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Filter Method - easiest but not the best way\n",
    "print(X[[\"sex\",\"relationship\"]].corr())\n",
    "\n",
    "X = X.drop('relationship',axis=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACKWARD ELIMINATION (wrapper method 1)\n",
    "\n",
    "- 1) We select a significance level (SL) to stay in the model\n",
    "- 2) Fit the full model with all possible predictors\n",
    "- 3) Loop (while p-value < SL)\n",
    "    - 4) Consider the predictor with highest p-value. \n",
    "    - 5) Remove the predictor\n",
    "    - 6) Fit model without this variable\n",
    "    \n",
    "So basically we are feeding all the possible features to the model and then proceed to iteratively remove \n",
    "the worst performing features one by one. The metric to evaluate feature performance is pvalue above 0.05 (to keep the feature).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmax = 1\n",
    "SL = 0.05 # the smaller the SL, the more features will be removed\n",
    "\n",
    "while (len(X.columns) > 0):\n",
    "    \n",
    "    p_values = []\n",
    "    \n",
    "    Xtemp = X[X.columns]\n",
    "    Xtemp = sm.add_constant(Xtemp)\n",
    "    model = sm.OLS(Y,Xtemp).fit()\n",
    "    \n",
    "    p = pd.Series(model.pvalues.values[1:],index = cols)      \n",
    "    p_max = max(p)\n",
    "    feature_pmax = p.idxmax()\n",
    "    \n",
    "    if(p_max > SL):\n",
    "        cols.remove(feature_pmax)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print('Features Selected: ', cols)\n",
    "\n",
    "X = X[cols]\n",
    "\n",
    "\"\"\"\n",
    "#If we just want to perform OLS regression to find out interesting stats:\n",
    "X1 = sm.add_constant(X)\n",
    "model = sm.OLS(Y, X1).fit()\n",
    "display(model.summary())\n",
    "print(model.pvalues)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in our case, backward elimination is telling us that we should remain with all the features. No need to drop any of them :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FORWARD SELECTION (wrapper method 2)\n",
    "\n",
    "Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs1 = sfs(LinearRegression(), k_features=len(X.columns) , forward=True , scoring='r2')\n",
    "sfs1 = sfs1.fit(X,Y)\n",
    "fig = plot_sfs(sfs1.get_metric_dict())\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(sfs1.k_feature_names_)\n",
    "\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "sfs1 = sfs(model_1 , k_features=9 , forward=True , scoring='r2')\n",
    "sfs1 = sfs1.fit(X_train,y_train)\n",
    "sfs1.k_feature_names_\n",
    "\n",
    "lr=LinearRegression().fit(X_train,y_train)\n",
    "y_pred_fs=lr.predict(X_test)\n",
    "r2_score(y_test,y_pred_fs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECURSIVE FEATURE ELIMINATION (wrapper method 3)\n",
    "\n",
    "It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. We need to find the optimum number of features, \n",
    "for which the accuracy is the highest. It then ranks the features based on the order of their elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nof_list=np.arange(1,len(df.columns)+1)            \n",
    "high_score=0\n",
    "\n",
    "nof=0           \n",
    "score_list =[]\n",
    "for n in range(len(nof_list)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3, random_state = 0)\n",
    "    model = LogisticRegression()\n",
    "    rfe = RFE(model,nof_list[n])\n",
    "    X_train_rfe = rfe.fit_transform(X_train,y_train)\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    model.fit(X_train_rfe,y_train)\n",
    "    score = model.score(X_test_rfe,y_test)\n",
    "    score_list.append(score)\n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        nof = nof_list[n]\n",
    "\n",
    "print(\"Optimum number of features: %d\" %nof)\n",
    "print(\"Score with %d features: %f\" % (nof, high_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we now the perfect number of features, we feed them to a RFE and get the final set of features given by RFE:'''\n",
    "cols = list(X.columns)\n",
    "model = LogisticRegression()\n",
    "\n",
    "rfe = RFE(model, nof)             \n",
    "\n",
    "X_rfe = rfe.fit_transform(X,Y)  \n",
    "\n",
    "model.fit(X_rfe,Y)              \n",
    "temp = pd.Series(rfe.support_,index = cols)\n",
    "selected_features_rfe = temp[temp==True].index\n",
    "\n",
    "X = X[selected_features_rfe]\n",
    "\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE SELECTION - EMBEDDED METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We iterate over the model training process and carefully \n",
    "extract those features which contribute the most \n",
    "to the training for a particular iteration, \n",
    "penalizing a feature given a threshold.\n",
    "Here we will do feature selection using Lasso regularization. \n",
    "If the feature is irrelevant, lasso penalizes its coefficient making it 0. \n",
    "Features with coefficient = 0 are removed.'''\n",
    "\n",
    "reg = LassoCV()\n",
    "reg.fit(X, y)\n",
    "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
    "print(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\n",
    "coef = pd.Series(reg.coef_, index = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "\n",
    "imp_coef = coef.sort_values()\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Feature importance using Lasso Model\");plt.show()\n",
    "\n",
    "#Here Lasso model has taken all the features except NOX, CHAS and INDUS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of SelectKBest (chi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLE OF USING SICKIT LEARN TO SELECT THE TWO BEST FEATURES\n",
    "#https://github.com/knathanieltucker/bit-of-data-science-and-scikit-learn/blob/master/notebooks/FeatureSelection.ipynb\n",
    "#https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "\n",
    "fit = bestfeatures.fit(X,np.ravel(y))\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(10,'Score'))  #print 10 best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Example of ExtRaTreesClassifier() **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE SELECTION USING EXTRA TREES CLASSIFIER\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "print(model.feature_importances_)\n",
    "feat_importances = pd.Series(model.feature_importances_, index = X.columns)\n",
    "feat_importances.nlargest(10).plot(kind = 'barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Example of RFECV(RandomForestClassifier()) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RECURSIVE FEATURE ELIMINATION with cross validation (using Random Forest)\n",
    "'''Assigns weights to features (e.g., the coefficients of a linear model). \n",
    "Features whose weights are the smallest are pruned out.'''\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "m = RFECV(RandomForestClassifier(), scoring = 'accuracy')\n",
    "m.fit(X,y)\n",
    "m.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of SelectFromModel (LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE SELECTION USING SelectFromModel AND SVC\n",
    "\n",
    "'''features are considered unimportant and removed, \n",
    "if the corresponding coef_ or featureimportances values \n",
    "are below the provided threshold parameter.\n",
    "Available heuristics are “mean”, “median” \n",
    "and float multiples of these like “0.1*mean”.'''\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "m = SelectFromModel(LinearSVC(C=0.01, penalty='l1', dual=False))\n",
    "m.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Example of SelectFromModel (LassoCV) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE SELECTION USING SelectFromModel AND LASSO\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "m = SelectFromModel(LassoCV(cv = 5))\n",
    "m.fit(X,y)\n",
    "#m.transform(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_pca = PCA(n_components=len(X.columns))\n",
    "Y_sklearn = sklearn_pca.fit_transform(X)\n",
    "\n",
    "cum_sum = sklearn_pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "sklearn_pca.explained_variance_ratio_[:10].sum()\n",
    "\n",
    "cum_sum = cum_sum*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "plt.bar(range(len(X.columns)), cum_sum, label='Cumulative _Sum_of_Explained _Varaince', color = 'b',alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In this case every feature adds significant variance to the data, so it's not a good idea to make dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models and ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models, to make predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FULL DATA CLEANSING AND CLASSIFICATION PIPELINE¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data from Titanic dataset.\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/amueller/'\n",
    "               'scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv')\n",
    "\n",
    "#We create the preprocessing pipelines for both numeric and categorical data\n",
    "numeric_features = ['age','fare']\n",
    "\n",
    "numeric_transformer = Pipeline(steps = [\n",
    "    ('imputer', SimpleImputer(strategy = 'constant', fill_value = 'missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers = [\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "#Append classifier to preprocessing pipeline\n",
    "#Now we have a full prediction pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(solver='lbfgs'))])\n",
    "\n",
    "X = df.drop('survived', axis = 1)\n",
    "y = df['survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format to get predictions from new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSIONALITY REDUCTION - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLE OF DIM REDUCTION WITH PCA\n",
    "#We first scale the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df)\n",
    "scaled_data = scaler.transform(df)\n",
    "#scaled_data.shape\n",
    "\n",
    "#Apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2) #2 is the number of dimension we want\n",
    "pca.fit(scaled_data)\n",
    "x_pca = pca.transform(scaled_data)\n",
    "#scaled_data.shape\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(x_pca[:,0], x_pca[:,1],c=df['target_col'])\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second principal component')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical ML implementation for a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://www.andreagrandi.it/2018/04/14/machine-learning-pima-indians-diabetes/\n",
    "# Import all the algorithms we want to test\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Prepare an array with all the algorithms\n",
    "models = []\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('LSVC', LinearSVC()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('RFC', RandomForestClassifier()))\n",
    "models.append(('SVM', SVC(gamma = 'auto')))\n",
    "\n",
    "#REGRESSION ALGORITHMS\n",
    "#models.append(('LR', LogisticRegression()))\n",
    "#models.append(('DTR', DecisionTreeRegressor()))\n",
    "#models.append(('SGDRegressor', linear_model.SGDRegressor())) \n",
    "#models.append(('BayesianRidge', linear_model.BayesianRidge()))\n",
    "#models.append(('LassoLars', linear_model.LassoLars())) \n",
    "#models.append(('ARDRegression', linear_model.ARDRegression())) \n",
    "#models.append(('PassiveAggressiveRegressor', linear_model.PassiveAggressiveRegressor())) \n",
    "#models.append(('TheilSenRegressor', linear_model.TheilSenRegressor()))\n",
    "#models.append(('LinearRegression', linear_model.LinearRegression())) \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "num_folds = 3\n",
    "scoring = 'accuracy'\n",
    "\n",
    "# Prepare the configuration to run the test\n",
    "seed = 7\n",
    "results = []\n",
    "names = []\n",
    "X = train_set_scaled\n",
    "y = train_set_labels\n",
    "\n",
    "# Every algorithm is tested and results are\n",
    "# collected and printed\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(\n",
    "        n_splits=3, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(\n",
    "        model, X, y, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (\n",
    "        name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n",
    "\n",
    "#ONCE CHOSEN THE ALGORITHM, WE DO SCALING, PARAM GRID K FOLD AND GRID SEARCH #################\n",
    "# Build a scaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "\n",
    "# Build parameter grid\n",
    "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n",
    "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "param_grid = dict(C=c_values, kernel=kernel_values)\n",
    "\n",
    "# Build the model\n",
    "model = SVC()\n",
    "kfold = KFold(n_splits = num_folds, random_state = seed)\n",
    "grid = GridSearchCV(estimator = model, param_grid = param_grid, scoring = scoring, cv = kfold)\n",
    "grid_result = grid.fit(rescaledX, y_train)\n",
    "\n",
    "# Show the results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "#MODEL IS NOW READY TO BE SAVED AND USED\n",
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding best parameters with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'classifier__C': [0.1, 1.0, 10, 100]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=10, iid=False)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print((\"best logistic regression from grid search: %.3f\"\n",
    "       % grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADIENT DESCENT = iterative technique to minimize complex loss function\n",
    "\n",
    "'''Two ways to combat overfitting:\n",
    "1. Use more training data. The more you have, the harder it is to overfit\n",
    "the data by learning too much from any single training example.\n",
    "\n",
    "2. Use regularization. Add in a penalty in the loss function for building\n",
    "a model that assigns too much explanatory power to any one feature or\n",
    "allows too many features to be taken into account.'''\n",
    "\n",
    "'''Regularization loss: how much we penalize the model \n",
    "for having large parameters that heavily weight certain features'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Basic Keras neural network\n",
    "#Source: https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "\n",
    "# load the dataset\n",
    "dataset = loadtxt(r'D:\\Downloads\\DATASETS\\general\\pima_indian_dataset.csv', delimiter=',')\n",
    "\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X, y, epochs=20, batch_size=40)\n",
    "\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
